properties([
  buildDiscarder(
    logRotator(numToKeepStr: '15')
  ),
  disableConcurrentBuilds(),
  gitLabConnection('Jenkins-Gitlab')
])

// AWS vars
def awsRegion = 'eu-west-2'
def ecrAccountName = 'live-mgmt'


// Jenkins variables
def jobName = "${env.JOB_NAME}"
def branchName = "${env.BRANCH_NAME}"
// simplifiedBranchName is used for namespaces, so removing feature prefix, converting to lower case and truncating...
simplifiedBranchName = branchName.replace("feature/", "").toLowerCase()
if ( simplifiedBranchName.length() > 24 ){
  echo "truncating branch name for use in namespaces ..."
  simplifiedBranchName = simplifiedBranchName.substring(0,23)
}
def jenkinsUrl = "${env.BUILD_URL}"
def jenkinsScriptsDir = 'VSMT_SETCHKS_APP/deployment/scripts'
def jobFolder = "vsmt_deployment"
// def nodeLabel = 'any'

def commitHash


// App & service Team vars
def appName = "vsmt-setchks-app"
def serviceTeam
def serviceTeamEnvName = 'dev'
def vsmt_setchks_app_version
def version ='latest'
def dockerDir = 'docker'
def dockerImage = 'vsmt-setchks-app'

def liveMgmtAccountId
def currentAccountId

// k8s vars
def appK8sDir = 'VSMT_SETCHKS_APP/deployment/k8s'
def appNamespace = "${appName}-${simplifiedBranchName}-${serviceTeamEnvName}"
def deploymentName = "${appName}-deploy"
def serviceName = "${appName}-svc"
def servicePort = 5000
def targetPort = 5000
def securityPolicy = "ELBSecurityPolicy-TLS-1-2-2017-01"
def saRole = "vsmt_host_role"

def albIngressName = "lb" // used in R53 record
def albScheme = 'internet-facing' // 'internal' or 'internet-facing'

//*******TAGS******************
//***** BILLING TAGS ******
def serviceTag = "vsmt"
def serviceOwnerTag = "jecr3"
def versionTag = "2.0"
def projectNameTag = "vsmt"
//******* Environment Tags *******
def dataClassificationTag = "1"
def environmentTag = "testing"
def publicFacingTag = "N"
//****** Application information Tags *******
def appNameTag = "nhsd-texas-sample pipeline-ingress"
def rvTag = "1"
def buildDateTag = new Date().format("dd-MM-yyyy")
def buildTimeTag = new Date().format("HH:mm")
def stackTag = "demo"
def toolTag = "k8s"
//******************************

def texasEnvName
def envDomain

def nodeLabel = 'master'
// ************************************************************************************
// Jenkins job folder/names must match the following convention EXACTLY:
//
// sample-pipeline/<service team name>/deploy to (live-lk8s-nonprod|live-lk8s-prod)
//
//   e.g. sample-pipelines/dspt/deploy to live-lk8s-nonprod
//
// ************************************************************************************

echo "BUILD_URL is ${BUILD_URL}"

// if ( jenkinsUrl ==~ /^http:\/\/texas-jenkins-v1:8080\/.*$/ ){
//   echo "This is Texas Jenkins MoM - changing nodeLabel to jenkins-slave"
//   nodeLabel = 'jenkins-slave'
// }

// Get service team and target environment from Jenkins job name
// if ( jobName ==~ /${jobFolder}\/([a-zA-Z\-_0-9]*)\/deploy to (dev-lk8s|dev-mgmt|test-lk8s|test-mgmt|live-lk8s-nonprod|live-mgmt|live-lk8s-prod)\/.*/ ){
if ( jobName ==~ /${jobFolder}\/vsmt\/deploy to (dev-lk8s|dev-mgmt|test-lk8s|test-mgmt|live-lk8s-nonprod|live-mgmt|live-lk8s-prod)\/.*/ ){
  def envMatch = jobName =~ /${jobFolder}\/([a-zA-Z\-_0-9]*)\/deploy to (dev-lk8s|dev-mgmt|test-lk8s|test-mgmt|live-lk8s-nonprod|live-mgmt|live-lk8s-prod)\/.*/
  serviceTeam = envMatch[0][1]
  texasEnvName = envMatch[0][2]
}
// else if ( ( texasEnvName != 'live-lk8s-nonprod' ) && ( serviceTeam != 'texas' ) ){
//   println "Only Texas can deploy this pipeline to environments other than non-prod"
//   currentBuild.result = 'FAILURE'
//   return
// }
else {
  println "Invalid job name"
  currentBuild.result = 'FAILURE'
  return
}

// Set additional config from target environment
if ( texasEnvName == 'live-lk8s-nonprod' ){
  envDomain = 'k8s-nonprod.texasplatform.uk'
} else if ( texasEnvName == 'live-lk8s-prod') {
  currentBuild.result = 'FAILURE'
  envDomain = 'k8s-prod.texasplatform.uk'
} else if ( texasEnvName == 'live-mgmt') {
  envDomain = 'mgmt.texasplatform.uk'
} else if ( texasEnvName == 'test-lk8s') {
  envDomain = 'k8s.texastest.uk'
} else if ( texasEnvName == 'test-mgmt') {
  envDomain = 'mgmt.texastest.uk'
} else if ( texasEnvName == 'dev-lk8s') {
  envDomain = 'k8s.texasdev.uk'
} else if ( texasEnvName == 'dev-mgmt') {
  envDomain = 'mgmt.texasdev.uk'
}

// The following variables are used for health checks after the deployment
// Note the trailing '.' !
hostedZoneName = "${envDomain}."
fqdn = "${appNamespace}-${albIngressName}.${envDomain}"
r53Record = "${fqdn}."
path = '/healthy'
url = "https://${fqdn}${path}"
searchText = 'Healthy'

def postFailure() {
    updateGitlabCommitStatus name: 'build', state: 'failed'
}

def postSuccess() {
    updateGitlabCommitStatus name: 'build', state: 'success'
}

node {
  try {

    stage('Print Config'){
      echo "################ JOB CONFIG ########################"
      echo "  serviceTeam is: ${serviceTeam}"
      echo "  serviceTeamEnvName is: ${serviceTeamEnvName}"
      echo "  texasEnvName is: ${texasEnvName}"
      echo "  hostedZoneName is: ${hostedZoneName}"
      echo "  FQDN is: ${fqdn}"
      echo "  Test URL is: ${url}"
      echo "  branchName is: ${branchName}"
      echo "  appName is: ${appName}"
      echo "  version is ${version}"
      echo "  jenkinsScriptsDir is: ${jenkinsScriptsDir}"
    
      echo "####################################################"
    }

    stage('Checkout') {
      echo 'Git repository checkout...'
      checkout scm
    }

    stage('Quality Checks'){
      echo "Placeholder for e.g. Sonarqube, git-secrets etc."
    }

    stage("Authenticate") {
        sh "chmod +x ${jenkinsScriptsDir}/*.sh"
        awsCreds = sh (
            script: "${jenkinsScriptsDir}/assume_role.sh ${texasEnvName} vsmt",
            returnStdout: true)
        awsAccessKeyId=awsCreds.split(',')[0].split('=')[1].trim()
        awsSecretAccessKey=awsCreds.split(',')[1].split('=')[1].trim()
        awsSessionToken=awsCreds.split(',')[2].split('=')[1].trim()
        withEnv([ "AWS_ACCESS_KEY_ID=${awsAccessKeyId}",
            "AWS_SECRET_ACCESS_KEY=${awsSecretAccessKey}",
            "AWS_SESSION_TOKEN=${awsSessionToken}"]) {
            kubeconfig = sh ( script: "${jenkinsScriptsDir}/kube_config.sh ${texasEnvName}", returnStdout: true).trim()
        accountSecrets = sh (
            script: """ aws secretsmanager get-secret-value --secret-id aws_account_ids \
            --region ${awsRegion} --query "SecretString" --output text """,
            returnStdout: true
        )
        accountSecretsMap = readJSON text: accountSecrets
        liveMgmtAccountId = accountSecretsMap['live-mgmt']
        currentAccountId = accountSecretsMap["${texasEnvName}"]
        echo "liveMgmt Account id = ${liveMgmtAccountId}"
        echo "current Account id = ${currentAccountId}"
        sleep 500
        }
    }

    stage('Build & Push'){
        commitHash = sh ( script: 'printf \$(git log --format=%h --abbrev=8 -n 1)', returnStdout: true).trim()
        vsmt_setchks_app_version = "${commitHash}"

      ecrImageName = "${liveMgmtAccountId}.dkr.ecr.${awsRegion}.amazonaws.com/${dockerImage}"
      sh """docker build --build-arg VERSION=${vsmt_setchks_app_version} --build-arg DEPLOYMENT_ENV=AWS -t ${ecrImageName}:${vsmt_setchks_app_version} -f VSMT_SETCHKS_APP/Dockerfile ."""
      // Authenticate against ECR
      sh """aws ecr get-login-password --region ${awsRegion} | docker login --username AWS --password-stdin ${liveMgmtAccountId}.dkr.ecr.eu-west-2.amazonaws.com"""
      // Create repo if it doesn't exist
      sh """aws ecr describe-repositories --repository-names ${dockerImage} || aws ecr create-repository --repository-name ${dockerImage}"""
      // Push image to ECR
      sh """docker push ${ecrImageName}:${vsmt_setchks_app_version}"""
    }

    stage('Scan Image') {
      sh "chmod +x ${jenkinsScriptsDir}/*.sh"
      sh "${jenkinsScriptsDir}/ecr_scan_image.sh ${dockerImage} ${version} false ${liveMgmtAccountId}"
    }

    stage("Authenticate to ${texasEnvName} AWS/k8s and tf apply") {
        withEnv(["MY_TOOL_DIR=${tool name: 'tfenv', type: 'com.cloudbees.jenkins.plugins.customtools.CustomTool'}"]){
            sh "tfenv install 1.6.0"
            sh "tfenv use 1.6.0"
        }
        awsCreds = sh (
            script: "${jenkinsScriptsDir}/assume_role.sh ${texasEnvName} vsmt",
            returnStdout: true)
        awsAccessKeyId=awsCreds.split(',')[0].split('=')[1].trim()
        awsSecretAccessKey=awsCreds.split(',')[1].split('=')[1].trim()
        awsSessionToken=awsCreds.split(',')[2].split('=')[1].trim()
        withEnv([ "AWS_ACCESS_KEY_ID=${awsAccessKeyId}",
            "AWS_SECRET_ACCESS_KEY=${awsSecretAccessKey}",
            "AWS_SESSION_TOKEN=${awsSessionToken}"]) {
            kubeconfig = sh ( script: "${jenkinsScriptsDir}/kube_config.sh ${texasEnvName}", returnStdout: true).trim()
            sh """aws sts get-caller-identity"""
            echo "kubeconfig is: ${kubeconfig}"
            dir ("${jenkinsScriptsDir}"){
                sh "./terraform.sh service_account ${texasEnvName} eu-west-2 apply"
            }
        }
    }

    stage("Deploy ${appName}"){
      // Prevent concurrent deployments
      lock ("${appNamespace}"){
        withEnv([ "AWS_ACCESS_KEY_ID=${awsAccessKeyId}",
          "AWS_SECRET_ACCESS_KEY=${awsSecretAccessKey}",
          "AWS_SESSION_TOKEN=${awsSessionToken}",
          "KUBECONFIG=${kubeconfig}"
          ])
        {
          wildcardCertDomainName = "*.${envDomain}"
          certArn = sh( script: """aws acm list-certificates | jq -r --arg jq_domainname ${wildcardCertDomainName} '.CertificateSummaryList[] | select(.DomainName == \$jq_domainname ) | .CertificateArn' """, returnStdout: true ).trim()

          echo "Texas wildcard cert ARN is: ${certArn}"

          dir (appK8sDir){
            // Create/update namespace
            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' namespace.yaml""")
            sh ("""kubectl apply -f namespace.yaml""")

            // Create/update vsmt_setchks_app deployment
            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' vsmt_setchks_app_deployment.yaml""")
            // Use | as a delimiter if the values contain /
            sh ("""sed -i 's|IMAGE_NAME_TO_BE_REPLACED|${ecrImageName}:${vsmt_setchks_app_version}|g' vsmt_setchks_app_deployment.yaml""")
            sh ("""sed -i 's/CONTAINER_NAME_TO_BE_REPLACED/${dockerImage}/g' vsmt_setchks_app_deployment.yaml""")
            sh ("""sed -i 's/DEPLOYMENT_NAME_TO_BE_REPLACED/${deploymentName}/g' vsmt_setchks_app_deployment.yaml""")
            sh ("""kubectl apply -f vsmt_setchks_app_deployment.yaml""")

            
            // Create/update vsmt_setchks_app service
            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' vsmt_setchks_app_service.yaml""")
            sh ("""sed -i 's/SERVICE_NAME_TO_BE_REPLACED/${serviceName}/g' vsmt_setchks_app_service.yaml""")
            sh ("""sed -i 's/PORT_TO_BE_REPLACED/${servicePort}/g' vsmt_setchks_app_service.yaml""")
            sh ("""sed -i 's/TARGET_PORT_TO_REPLACE/${targetPort}/g' vsmt_setchks_app_service.yaml""")
            sh ("""sed -i 's/SERVICE_SELECTOR_TO_BE_REPLACED/${deploymentName}/g' vsmt_setchks_app_service.yaml""")
            sh ("""kubectl apply -f vsmt_setchks_app_service.yaml""")

           
            // Create/update ingress (ALB)
            // Epochtime is used to verify whether the ingress has been updated since the previous deployment
            epochtime = sh( script: """ date +%s """, returnStdout: true ).trim()
            sh ("""sed -i 's/EPOCHTIME_TO_BE_REPLACED/${epochtime}/g' ingress.yaml""")
            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' ingress.yaml""")
            sh ("""sed -i 's/SERVICE_NAME_TO_BE_REPLACED/${serviceName}/g' ingress.yaml""")
            sh ("""sed -i 's/INGRESS_NAME_TO_BE_REPLACED/${albIngressName}/g' ingress.yaml""")
            sh ("""sed -i 's/PORT_TO_BE_REPLACED/${servicePort}/g' ingress.yaml""")
            sh ("""sed -i 's/ALB_INGRESS_TO_BE_REPLACED/${albIngressName}/g' ingress.yaml""")
            sh ("""sed -i 's/ALB_SCHEME_TO_BE_REPLACED/${albScheme}/g' ingress.yaml""")
            sh ("""sed -i 's/SECURITY_POLICY_TO_BE_REPLACED/${securityPolicy}/g' ingress.yaml""")
            // TAGS for ALB
            sh ("""sed -i 's/SERVICE_TAG_TO_BE_REPLACED/${serviceTag}/g' ingress.yaml""")
            sh ("""sed -i 's/SERVICE_OWNER_TAG_TO_BE_REPLACED/${serviceOwnerTag}/g' ingress.yaml""")
            sh ("""sed -i 's/VERSION_TAG_TO_BE_REPLACED/${versionTag}/g' ingress.yaml""")
            sh ("""sed -i 's/PROJECT_NAME_TAG_TO_BE_REPLACED/${projectNameTag}/g' ingress.yaml""")

            sh ("""sed -i 's/DATA_CLASSIFICATION_TAG_TO_BE_REPLACED/${dataClassificationTag}/g' ingress.yaml""")
            sh ("""sed -i 's/ENVIRONMENT_TAG_TO_BE_REPLACED/${environmentTag}/g' ingress.yaml""")
            sh ("""sed -i 's/PUBLIC_FACING_TAG_TO_BE_REPLACED/${publicFacingTag}/g' ingress.yaml""")

            sh ("""sed -i 's/APPLICATION_NAME_TAG_TO_BE_REPLACED/${appNameTag}/g' ingress.yaml""")
            sh ("""sed -i 's/RV_TAG_TO_BE_REPLACED/${rvTag}/g' ingress.yaml""")
            sh ("""sed -i 's/BUILD_DATE_TAG_TO_BE_REPLACED/${buildDateTag}/g' ingress.yaml""")
            sh ("""sed -i 's/BUILD_TIME_TAG_TO_BE_REPLACED/${buildTimeTag}/g' ingress.yaml""")
            sh ("""sed -i 's/STACK_TAG_TO_BE_REPLACED/${stackTag}/g' ingress.yaml""")
            sh ("""sed -i 's/TOOL_TAG_TO_BE_REPLACED/${toolTag}/g' ingress.yaml""")

            sh ("""sed -i 's|CERT_ARN_TO_BE_REPLACED|${certArn}|g' ingress.yaml""")

            sh ("""kubectl apply -f ingress.yaml""")

            sh ("""sed -i 's|ACCT_NUMBER_TO_BE_REPLACED|${currentAccountId}|g' sa.yaml""")
            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' sa.yaml""")
            sh ("""sed -i 's/SA_ROLE/${saRole}/g' sa.yaml""")

            sh ("""kubectl apply -f sa.yaml""")

            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' role.yaml""")
            sh ("""kubectl apply -f role.yaml""")

            sh ("""sed -i 's/NAMESPACE_TO_BE_REPLACED/${appNamespace}/g' role_binding.yaml""")
            sh ("""kubectl apply -f role_binding.yaml""")

          }
        }
      }
    }

    stage ("Test the endpoint"){
      withEnv([
          "AWS_ACCESS_KEY_ID=${awsAccessKeyId}",
          "AWS_SECRET_ACCESS_KEY=${awsSecretAccessKey}",
          "AWS_SESSION_TOKEN=${awsSessionToken}",
          "KUBECONFIG=${kubeconfig}"
      ])
      {
        // Wait for jenkins pod to be available - currently takes around 5 mins
        sh """${jenkinsScriptsDir}/check_pods.sh ${deploymentName} ${appNamespace} 5 10"""

        sleep 10 // to allow time for ingress to be created after pods ready

        // Ensure ingress is ready with updated epochtime annotation before attempting to query ALB health (otherwise may return old endpoint)
        sh """${jenkinsScriptsDir}/check_ingress.sh ${albIngressName} ${appNamespace} ${epochtime} 5 30"""

        loadBalancerDNSName = sh ( script: """kubectl get ingress ${albIngressName} -n ${appNamespace} -o json | jq -r '.status.loadBalancer.ingress[].hostname' """, returnStdout: true ).trim()
        echo "loadBalancerDNSName is ${loadBalancerDNSName}"

        loadBalancerArn = sh ( script: """aws elbv2 describe-load-balancers --region=${awsRegion} | jq -r --arg jq_lbdnsname ${loadBalancerDNSName} '.LoadBalancers[] | select(.DNSName == \$jq_lbdnsname) | .LoadBalancerArn'""", returnStdout: true ).trim()
        echo "loadBalancerArn is ${loadBalancerArn}"

        targetGroupArn = sh ( script: """aws elbv2 describe-target-groups --region=${awsRegion} | jq -r --arg jq_lbarn ${loadBalancerArn} '.TargetGroups[] | select(.LoadBalancerArns == [\$jq_lbarn]) | .TargetGroupArn'""", returnStdout: true ).trim()
        echo "targetGroupArn is ${targetGroupArn}"

        sh """${jenkinsScriptsDir}/check_alb.sh ${targetGroupArn} 30 20"""

        echo "HostedZoneName ${hostedZoneName}"
        hostedZoneId = sh( script: """ aws route53 list-hosted-zones | jq -r --arg jq_hostedzonename ${hostedZoneName} '.HostedZones[] | select(.Name == \$jq_hostedzonename) | .Id' """, returnStdout: true ).trim()
        echo "hostedZoneId is ${hostedZoneId}"

        // Note the trailing . on the target FQDN to check to for
        sh """${jenkinsScriptsDir}/check_r53.sh ${hostedZoneId} ${r53Record} ${loadBalancerDNSName}. 5 30"""

        // Only run curl against endpoint if it is publicly accessible
        if ( albScheme == 'internet-facing'){
          // Check endpoint using curl
          sh """${jenkinsScriptsDir}/curl_endpoint.sh ${url} \"${searchText}\" 5 30"""
        }
        else {
          echo '''Skipping curl check as endpoint is internal and won't be accessible'''
        }
      }
    }

    // stage("Destroy webapp") {
    //   // Prevent concurrent app ns destroys
    //   lock ("${appNamespace}"){
    //     withEnv([ "AWS_ACCESS_KEY_ID=${awsAccessKeyId}",
    //       "AWS_SECRET_ACCESS_KEY=${awsSecretAccessKey}",
    //       "AWS_SESSION_TOKEN=${awsSessionToken}",
    //       "KUBECONFIG=${kubeconfig}"
    //       ])
    //     {
    //       sh """kubectl delete ns ${appNamespace}"""
    //     }
    //   }
    // }

  // Post build result to GitLab
  } finally {
    def currentResult = currentBuild.result ?: 'SUCCESS'
    if (currentResult == 'SUCCESS') {
      postSuccess()
    } else {
      postFailure()
    }
  }
}
